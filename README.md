Day 1 log snippet:
Researched open APIs to grab data from and how they come to handle latency delays etc, and also researched about MM and typical approaches to grasp the challenge.
Implemented BinanceBook (takes diff in depth reconstruction via U/u sequence checks) and OKXBook (books5 has 5level snapshots). Binance logic deletes levels when their quantity is 0 which is standard procedure discussed online, and sequence gaps trigger a resync and to reconstruct. OKX simply overwrites the top 5 each update . Freshness is handled via the t_arrive_ns timestamp which is stored per update. Going to finish up the OKX logic next, and then handle the fair-price engine.

Day 2 log snippet:
Used prior day to build off of, and implemented a latency‑weighted cross‑venue fair mid to base our spread off of. Built a FairPriceEngine that drops any venue snapshot older than 0.5s and quotes around the blended mid. Added the Binance and OKX book snapshots: Binance via REST snapshot + U/u diffs (levels deleted on qty=0), OKX via books5 storing all 5 levels for imbalance/depth. Finished tweak to OKX to display mid and to capture 5 levels. Next im going to to bring the pipeline fully live and logging, then replace the fixed half‑spread with an adaptive spread (look into EWMA 1‑s vol + small impact buffer) and add inventory skew (Avellaneda–Stoikov style) with a simple 3‑regime switch before backtesting with markout/realized spread.

Day 3:
Built off the prior price model to make the spread dynamic off EWMA 1‑s vol + top‑5 imbalance, and added a simple inventory skew. Brought the connectors fully live: buffer‑first WS + REST snapshot sync (Binance U/u gate) and OKX books5 as 5‑level snapshots; switched REST to httpx.AsyncClient, added backoff + payload checks. Kept the latency‑weighted cross‑venue mid with a 0.5s freshness filter; need to compute imbalance from top‑5 if a venue doesn’t send it, persist inventory, tune λ/a/b/κ on tape, and add markout/realized‑spread backtests.

Day 4:
Short day working, fixed the binance connector to Binance.us to fix the issue with geolocation restrictions on binance.com. Implemented the official snapshot + U/u diff bootstrap with the bridging rule and unwrapped combined stream payloads. The venue now blends venues with 1/age + epsilon weights and drops anything staler than 0.5s successfully. This should allow for more up to date estimates of fair mid price due to newest information carrying more weight.

Day 5:
Finished the pieces that grab top 5 levels and compute the top-5 imbalance on both venues, this now feeds the spread. The point is simple if one sided depth near the top often precedes the next short horizon move, so we widen to reduce pick‑off risk. next im persisting inventory and applying a small inventory lean to the quote center (Avellaneda–Stoikov‑style) to cut one‑sided exposure. I hope I can start on tape tuning my parameters and plugging them in. After a few hours of tape, i will upgrade the recency blend to a state‑space (Kalman) fair value so older/wider venue quotes are down‑weighted via measurement noise (from age and spread). I then will convert uncertainity to half spread and keep skew/impact active e.g high uncertainity we widen quotes. Goal is to have a latency aware pipeline that can map uncertainity into spread and be able to demonstrate fill quality via a 15s/1m markout and realized spread versus a fixed spread baseline

Day 6:
Built the core building blocks: a 1‑D Kalman filter to fuse Binance and OKX mids into a single, latency‑aware fair value; an EWMA module tracking 1‑s volatility; an inventory manager to lean our quotes away from one‑sided risk; and a recorder that logs every quote (bids, asks, mid, spread, imbalance, σ, etc.) for replay. These tools let us replace yesterday’s static spread with a truly reactive model that adapts in real time to changing market dynamics. Going to integrate these pieces into our FairPriceEngine so it not only copmputes uncertainity but uses it, alongside imbalance and inventory to generate smarter, more adaptive quotes that outperform our baseline. On the strategy side, i have been coming up with ideas, and will fit a baseline model on our engienered LOB features and then residualize that models errors to uncover orthogonal signals, new alphas hidden in the "noise" or information unexplained, which i will look for in sensors at t+1 (my assumption is that if there is correlation which e.g OFI with delta of mid price, then the same is true at t+1, so if we model t+1 OFI and then remove the redundancy overlap between our prior features, we have orthogonal information to predict the noise, and improve our SNR (signal to noise)) i will compare both models against a random and fixed-rule benchmark. if time permits, id be interested in comparing with a proof of concept CNN + LSTM (DeepLOB style) from this paper https://www.sciencedirect.com/science/article/pii/S0169207024000062?utm_source=chatgpt.com#sec3.2.2

Day 7:
In this challenge, I researched and learn what MM was and entailed and gained great insight, although my results were not what I would have liked to see, I didn't come into this thinking I could make a professional grade MM in 7 days, nor have the time to record data to train on. I approached this project by first learning and researching the main principles of MM and what it entails, to put it simply I thought of it as trying to map our risk and uncertainity to provide liquidity as optimal as possible. I looked into parameters that would help, how to model our uncertainity, how to adjust the spread, and to measure our risk. I felt like I did a good job in providing the model with the parameters and such, but I think the fine tuning and such was only done on less than a hour of data so it is hard to really determine the correct results, considering the testing data and training are just noise without months of data to run on. I would of have liked to look more into what I had done in mid frequency modeling, which is trying to map regimes, and understand how parameters/relationships may change, I feel like uncovering this would be insightful for MM to adjust our parameters in these times. For the strategy I didn't have enough time really, to do anything too insightful and had to use bare minimum features with no creative outlet do to lack of time, and focusing too much into researching about MM principles. In hindsight I should have balanced the two challenges, but I had no prior knowledge to MM and wanted to focus my energy there.

Overall I did the following:

Kalman-Filter Fair-Value Fusion – treat the latent fair price as a 1-D state updated by each venues mid quote via a sequential Kalman filter. Measurement noise of the feed scales with age and spread so stale/wide feeds are down weighted. https://etheses.lse.ac.uk/4066/1/Zabaljauregui_Optimal-market-making.pdf?

EWMA Volatility for Spread Scaling – Short horizon volatility is computed via a 1-s EWMA on mid returns smoothing noise but reacting to volatility spikes.

Inventory Skew (Avellaneda–Stoikov Style)
We lean mid by -k * inventory so the quotes naturally push us back towards flat when inventory drifts

Data Record and Pipeline
Every quote (bids5, asks5, mid, fair_mid, bid, ask, sigma, imbalance, etc.) is dumped to JSONL. Stored for easy use of backtesting and such, too bad couldn't store and run computer over night for more data.

Trade-offs & Challenges

Data Storage & Latency
Building a persistent recorder cost time early on—writing raw JSONL. Less data to work with

Imbalance & Adverse Selection
Used top-5 Order Flow Imbalance (OFI) as an impact proxy. OFI has documented predictive power but limited Sharpe when used alone; our simulations confirmed negative selection in thin books without additional corrections

Partial Information & Sequence Gaps
Handling Binance’s and OKX streams took time to pick up and debug the code, had to browse all over the internet to find good sources. Then had to figure out the correct logic in storing and running through the data to avoid stale feeds and gaps.

Implementation & Bugs Solved
Binancebook, changed to a simpler stateless stream to avoid errors seen before with REST. Dropped either venue with 0.5s or more to avoid hanging quotes.

I had bugs with a lot of the libraries and dataclass etc, I had to fix the data and make sure the files were all correct syntax and use. For the trading strategy this is why I kept it in one class to make it simpler and not complicated things. I had trouble when I was training the model as well it was taking 0 trades, I figured out becasue in my fair price engine I was compared time now versus the prior, well the prior was from training data which was > 1 day old. So the fresh wasn't storing anything. I as well was having issues with the logic for the formulas, and had to adjust to make sure the spread wasn't too wide, before my parameters were quoting way too big and I realized I had my uncertainity values too high.

Approach to the trading:
I wanted to compare a baseline model versus other features and sensors at the time t+1. My approach was to find current correlations at time t, such as OFI and current delta of mid price, too test we could go through and model OFI at t+1 and delta mid t+1, or t+ h , whatever horizon bar we are examining in. The point was, if our sensor at t + h correlates with delta in mid price at t+h, we can then increase our SNR, signal to noise, for our model, by modeling the noise from our baseline model with this new orthogonal information. To do this we need to make sure when we predict the sensor at time t + h , we remove the redundant information, so we model X (our prior info) on the new info Z(which can be OFI at t + h) and then what is not explained by X, is our new orthogonal information. We then use this to train on the error of the base line e , to model and predict the noise , that is not counted for in our model using X. This was my approach to refine with more intuitive features. Unfortuantatley like I stated earlier I ran out of time to go fully in depth on this, as I wasted so much time, storing and gathering data and working on the MM.

Lessons learned
I learned MM is a partial information game similar to poker, which I love to play. We have to value stale, nosiy feeds and residual imbalance isn't enough. I also realized that data engineering dominates early effort, robust recording and replay are pre reqs to a credible backtest. I also learned that I need to focus more on one parameter tuning at a time, to see what affect, and model the affect so that we can isolate certain information and make sure we are performing optimally. If I had more time I would of spent robust time on each parameter tuning, I would have more data to test on and then make sure we are able to clearly understand the impact of each one. I learned as well the impact of how to adjust in incomplete or risky scenarios, and how it is super important that we are not accounting for the risk at hand, and our inventory. We not only have to act optiamlly in the markets but for our inventory constraint as well. I also understood before the importance of walk forward testing to avoid look ahead bias, I wish I would have been able to simulate better with execution costs. In my model the strategy shows a R^2 of ~ 0.11 which shows it is not just noise, however the sensor at time t+h we try to model actually makes our model more prone to noise and looses robustness. However I wouldn't make any conclusions based on the fact this is less than 2 hours of training data at HFT data. I just had to put somethign together to see if we could replicate good modeling practices. 

