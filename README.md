Day 1 log snippet:
Researched open APIs to grab data from and how they come to handle latency delays etc, and also researched about MM and typical approaches to grasp the challenge.
Implemented BinanceBook (takes diff in depth reconstruction via U/u sequence checks) and OKXBook (books5 has 5level snapshots). Binance logic deletes levels when their quantity is 0 which is standard procedure discussed online, and sequence gaps trigger a resync and to reconstruct. OKX simply overwrites the top 5 each update . Freshness is handled via the t_arrive_ns timestamp which is stored per update. Going to finish up the OKX logic next, and then handle the fair-price engine.

Day 2 log snippet:
Used prior day to build off of, and implemented a latency‑weighted cross‑venue fair mid to base our spread off of. Built a FairPriceEngine that drops any venue snapshot older than 0.5s and quotes around the blended mid. Added the Binance and OKX book snapshots: Binance via REST snapshot + U/u diffs (levels deleted on qty=0), OKX via books5 storing all 5 levels for imbalance/depth. Finished tweak to OKX to display mid and to capture 5 levels. Next im going to to bring the pipeline fully live and logging, then replace the fixed half‑spread with an adaptive spread (look into EWMA 1‑s vol + small impact buffer) and add inventory skew (Avellaneda–Stoikov style) with a simple 3‑regime switch before backtesting with markout/realized spread.

Day 3:
Built off the prior price model to make the spread dynamic off EWMA 1‑s vol + top‑5 imbalance, and added a simple inventory skew. Brought the connectors fully live: buffer‑first WS + REST snapshot sync (Binance U/u gate) and OKX books5 as 5‑level snapshots; switched REST to httpx.AsyncClient, added backoff + payload checks. Kept the latency‑weighted cross‑venue mid with a 0.5s freshness filter; need to compute imbalance from top‑5 if a venue doesn’t send it, persist inventory, tune λ/a/b/κ on tape, and add markout/realized‑spread backtests.

Day 4:
Short day working, fixed the binance connector to Binance.us to fix the issue with geolocation restrictions on binance.com. Implemented the official snapshot + U/u diff bootstrap with the bridging rule and unwrapped combined stream payloads. The venue now blends venues with 1/age + epsilon weights and drops anything staler than 0.5s successfully. This should allow for more up to date estimates of fair mid price due to newest information carrying more weight.

Day 5:
Finished the pieces that grab top 5 levels and compute the top-5 imbalance on both venues, this now feeds the spread. The point is simple if one sided depth near the top often precedes the next short horizon move, so we widen to reduce pick‑off risk. next im persisting inventory and applying a small inventory lean to the quote center (Avellaneda–Stoikov‑style) to cut one‑sided exposure. I hope I can start on tape tuning my parameters and plugging them in. After a few hours of tape, i will upgrade the recency blend to a state‑space (Kalman) fair value so older/wider venue quotes are down‑weighted via measurement noise (from age and spread). I then will convert uncertainity to half spread and keep skew/impact active e.g high uncertainity we widen quotes. Goal is to have a latency aware pipeline that can map uncertainity into spread and be able to demonstrate fill quality via a 15s/1m markout and realized spread versus a fixed spread baseline